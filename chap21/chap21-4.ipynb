{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install basic toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq opencv-contrib-python\n",
    "!pip install -qq controlnet_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from pathlib import Path\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from diffusers import (\n",
    "    ControlNetModel,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UniPCMultistepScheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://media.discordapp.net/attachments/1092064974794391644/1194275237538316328/image.png?ex=65afc27d&is=659d4d7d&hm=7bdb56abb8dae3a36e7a7943f3f6b08354f830ee843ef3703aac6e64b06814da&=&format=webp&quality=lossless&width=898&height=874\"\n",
    ")\n",
    "\n",
    "# Invert the black and white colors of the image, the effect will be better\n",
    "inverted_image = ImageOps.invert(image)\n",
    "inverted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = inverted_image\n",
    "\n",
    "depth_estimator = pipeline('depth-estimation')\n",
    "image = depth_estimator(image)['depth']\n",
    "image = np.array(image)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "control_image = Image.fromarray(image)\n",
    "\n",
    "control_image.save(\"control.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ControlNet1.1's depth model\n",
    "checkpoint = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"jungle, masterpiece, best quality\", \"leaves, masterpiece, best quality\", \"waves, masterpiece, best quality\", \"cloud, masterpiece, best quality\"]\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images\n",
    "\n",
    "# image.save('image_out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to display image\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "image_grid(image, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the effect without color inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://media.discordapp.net/attachments/1092064974794391644/1194275237538316328/image.png?ex=65afc27d&is=659d4d7d&hm=7bdb56abb8dae3a36e7a7943f3f6b08354f830ee843ef3703aac6e64b06814da&=&format=webp&quality=lossless&width=898&height=874\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"High detail,(detailed light:0.8),rainwater,Ambient light,3d rendering,realistic,8k,cinema light,(Osmanthus fragrans:1.4),Osmanthus fragrans,Water drop,(rain:1.2),(Fuzzy background:1.2), Above water, Plant in soil, pond\"\n",
    "\n",
    "depth_estimator = pipeline('depth-estimation')\n",
    "image = depth_estimator(image)['depth']\n",
    "image = np.array(image)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "control_image = Image.fromarray(image)\n",
    "\n",
    "control_image.save(\"control_noinvert.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
