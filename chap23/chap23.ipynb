{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Python packages needed\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "from subprocess import getoutput\n",
    "from IPython.utils import capture\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set various paths needed for the project, such as base model storage path, LoRA model training output path, etc.\n",
    "root_dir = os.path.abspath(\"/content\")\n",
    "deps_dir = os.path.join(root_dir, \"deps\")\n",
    "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
    "training_dir = os.path.join(root_dir, \"LoRA\")\n",
    "pretrained_model = os.path.join(root_dir, \"pretrained_model\")\n",
    "vae_dir = os.path.join(root_dir, \"vae\")\n",
    "config_dir = os.path.join(training_dir, \"config\")\n",
    "\n",
    "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "tools_dir = os.path.join(repo_dir, \"tools\")\n",
    "finetune_dir = os.path.join(repo_dir, \"finetune\")\n",
    "\n",
    "# TODO：In a new notebook, this code can be deleted\n",
    "for store in [\n",
    "    \"root_dir\",\n",
    "    \"deps_dir\",\n",
    "    \"repo_dir\",\n",
    "    \"training_dir\",\n",
    "    \"pretrained_model\",\n",
    "    \"vae_dir\",\n",
    "    \"accelerate_config\",\n",
    "    \"tools_dir\",\n",
    "    \"finetune_dir\",\n",
    "    \"config_dir\",\n",
    "]:\n",
    "    with capture.capture_output() as cap:\n",
    "        del cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
    "bitsandytes_main_py = \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py\"\n",
    "branch = \"\"\n",
    "install_xformers = True\n",
    "verbose = False\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "\n",
    "def write_file(filename, contents):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "\n",
    "def clone_repo(url):\n",
    "    if not os.path.exists(repo_dir):\n",
    "        os.chdir(root_dir)\n",
    "        !git clone {url} {repo_dir}\n",
    "    else:\n",
    "        os.chdir(repo_dir)\n",
    "        !git pull origin {branch} if branch else !git pull\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    s = getoutput('nvidia-smi')\n",
    "\n",
    "    !pip install {'-q' if not verbose else ''} --upgrade -r requirements.txt\n",
    "    !pip install {'-q' if not verbose else ''} torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
    "\n",
    "    if install_xformers:\n",
    "        !pip install {'-q' if not verbose else ''} xformers==0.0.19 triton==2.0.0 -U\n",
    "\n",
    "    from accelerate.utils import write_basic_config\n",
    "\n",
    "    if not os.path.exists(accelerate_config):\n",
    "        write_basic_config(save_location=accelerate_config)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.chdir(root_dir)\n",
    "\n",
    "    for dir in [\n",
    "        deps_dir,\n",
    "        training_dir,\n",
    "        config_dir,\n",
    "        pretrained_model,\n",
    "        vae_dir\n",
    "    ]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    clone_repo(repo_url)\n",
    "\n",
    "    os.chdir(repo_dir)\n",
    "    !apt --fix-broken install\n",
    "    !apt install aria2 {'-qq' if not verbose else ''}\n",
    "\n",
    "    install_dependencies()\n",
    "    time.sleep(3)\n",
    "\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "\n",
    "    cuda_path = \"/usr/local/cuda-11.8/targets/x86_64-linux/lib/\"\n",
    "    ld_library_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "    os.environ[\"LD_LIBRARY_PATH\"] = f\"{ld_library_path}:{cuda_path}\"\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"/content/pretrained_model/moyou.safetensors\"\n",
    "!wget -c https://civitai.com/api/download/models/143001 -O $pretrained_model_name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = os.path.join(root_dir, \"LoRA/train_data/hb_cartoon\")\n",
    "os.makedirs(train_data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Your train data directory : {train_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Heben's images\n",
    "# If you want to train LoRA with your own images, you can upload your images to /content/LoRA/train_data/hb_cartoon path, skip this code.\n",
    "!wget https://github.com/yipingw/ai_painting/raw/main/chap23/data/herburn_images.tar\n",
    "\n",
    "# Extract Heben images to target training path\n",
    "!tar -xvf herburn_images.tar -C /content/LoRA/train_data/\n",
    "\n",
    "train_data_dir = os.path.join(root_dir, \"LoRA/train_data/herburn_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Used to display images\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "pths = glob(r\"/content/LoRA/train_data/herburn_images/*\")\n",
    "imgs = []\n",
    "for pth in pths:\n",
    "  img = Image.open(pth)\n",
    "  imgs.append(img.resize((512, 512)))\n",
    "\n",
    "image_grid(imgs[:4], 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation instructions: You need to place the photos you prepared in the path above, such as the path here: /content/LoRA/train_data/hb_cartoon. Just drag the images to the folder! Make sure the file upload is complete, then go to next step. 10+ images, as many as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BLIP model to add prompts to your images, for training.\n",
    "# BLIP is a multimodal generative algorithm, input image, get image prompt description information.\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "\n",
    "batch_size = 8\n",
    "max_data_loader_n_workers = 2\n",
    "beam_search = True\n",
    "min_length = 5\n",
    "max_length = 75\n",
    "recursive = False\n",
    "verbose_logging = True\n",
    "\n",
    "config = {\n",
    "    \"_train_data_dir\" : train_data_dir,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"beam_search\" : beam_search,\n",
    "    \"min_length\" : min_length,\n",
    "    \"max_length\" : max_length,\n",
    "    \"debug\" : verbose_logging,\n",
    "    \"caption_extension\" : \".caption\",\n",
    "    \"max_data_loader_n_workers\" : max_data_loader_n_workers,\n",
    "    \"recursive\" : recursive\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python make_captions.py {args}\"\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"hb_pro\"\n",
    "vae = \"\" # Use VAE in base model\n",
    "output_dir = os.path.join(root_dir, \"LoRA/output/hb_pro\")\n",
    "\n",
    "sample_dir = os.path.join(output_dir, \"sample\")\n",
    "for dir in [output_dir, sample_dir]:\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "print(\"Project Name: \", project_name)\n",
    "print(\n",
    "    \"Pretrained Model Path: \", pretrained_model_name_or_path\n",
    ") if pretrained_model_name_or_path else print(\"No Pretrained Model path specified.\")\n",
    "\n",
    "print(\"VAE Path: \", vae) if vae else print(\"No VAE path specified.\")\n",
    "print(\"Output Path: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to preprocess data, process our training data, regularization data into dataloader that the training model can use\n",
    "# Data augmentation is a very critical operation, such as image flipping, image color disturbance, image transition towards target style, etc. These can be tuned later.\n",
    "\n",
    "import os\n",
    "import toml\n",
    "import glob\n",
    "\n",
    "dataset_repeats = 20\n",
    "activation_word = \"\"\n",
    "caption_extension = \".caption\"\n",
    "resolution = 512\n",
    "flip_aug = True\n",
    "keep_tokens = 0\n",
    "\n",
    "def parse_folder_name(folder_name, default_num_repeats, default_class_token):\n",
    "    folder_name_parts = folder_name.split(\"_\")\n",
    "\n",
    "    if len(folder_name_parts) == 2:\n",
    "        if folder_name_parts[0].isdigit():\n",
    "            num_repeats = int(folder_name_parts[0])\n",
    "            class_token = folder_name_parts[1].replace(\"_\", \" \")\n",
    "        else:\n",
    "            num_repeats = default_num_repeats\n",
    "            class_token = default_class_token\n",
    "    else:\n",
    "        num_repeats = default_num_repeats\n",
    "        class_token = default_class_token\n",
    "\n",
    "    return num_repeats, class_token\n",
    "\n",
    "def find_image_files(path):\n",
    "    supported_extensions = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
    "    return [file for file in glob.glob(path + '/**/*', recursive=True) if file.lower().endswith(supported_extensions)]\n",
    "\n",
    "def process_data_dir(data_dir, default_num_repeats, default_class_token, is_reg=False):\n",
    "    subsets = []\n",
    "\n",
    "    images = find_image_files(data_dir)\n",
    "    if images:\n",
    "        subsets.append({\n",
    "            \"image_dir\": data_dir,\n",
    "            \"class_tokens\": default_class_token,\n",
    "            \"num_repeats\": default_num_repeats,\n",
    "            **({\"is_reg\": is_reg} if is_reg else {}),\n",
    "        })\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            images = find_image_files(folder_path)\n",
    "\n",
    "            if images:\n",
    "                num_repeats, class_token = parse_folder_name(folder, default_num_repeats, default_class_token)\n",
    "\n",
    "                subset = {\n",
    "                    \"image_dir\": folder_path,\n",
    "                    \"class_tokens\": class_token,\n",
    "                    \"num_repeats\": num_repeats,\n",
    "                }\n",
    "\n",
    "                if is_reg:\n",
    "                    subset[\"is_reg\"] = True\n",
    "\n",
    "                subsets.append(subset)\n",
    "\n",
    "    return subsets\n",
    "\n",
    "\n",
    "train_subsets = process_data_dir(train_data_dir, dataset_repeats, activation_word)\n",
    "print(train_subsets)\n",
    "# reg_subsets = process_data_dir(reg_data_dir, dataset_repeats, activation_word, is_reg=True)\n",
    "\n",
    "# subsets = train_subsets + reg_subsets\n",
    "subsets = train_subsets\n",
    "\n",
    "config = {\n",
    "    \"general\": {\n",
    "        \"enable_bucket\": True,\n",
    "        \"caption_extension\": caption_extension,\n",
    "        \"shuffle_caption\": True,\n",
    "        \"keep_tokens\": keep_tokens,\n",
    "        \"bucket_reso_steps\": 64,\n",
    "        \"bucket_no_upscale\": False,\n",
    "    },\n",
    "    \"datasets\": [\n",
    "        {\n",
    "            \"resolution\": resolution,\n",
    "            \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
    "            \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
    "            \"caption_dropout_rate\": 0,\n",
    "            \"caption_tag_dropout_rate\": 0,\n",
    "            \"caption_dropout_every_n_epochs\": 0,\n",
    "            \"flip_aug\": flip_aug,\n",
    "            \"color_aug\": False,\n",
    "            \"face_crop_aug_range\": None,\n",
    "            \"subsets\": subsets,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
    "\n",
    "for key in config:\n",
    "    if isinstance(config[key], dict):\n",
    "        for sub_key in config[key]:\n",
    "            if config[key][sub_key] == \"\":\n",
    "                config[key][sub_key] = None\n",
    "    elif config[key] == \"\":\n",
    "        config[key] = None\n",
    "\n",
    "config_str = toml.dumps(config)\n",
    "\n",
    "with open(dataset_config, \"w\") as f:\n",
    "    f.write(config_str)\n",
    "\n",
    "print(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can provide pre-trained LoRA model\n",
    "# Set learning rate for text_encoder and UNet respectively\n",
    "\n",
    "network_category = \"LoRA\"\n",
    "\n",
    "conv_dim = 32\n",
    "conv_alpha = 16\n",
    "network_dim = 32\n",
    "network_alpha = 16\n",
    "network_weight = \"\"\n",
    "network_module = \"networks.lora\"\n",
    "network_args = \"\"\n",
    "\n",
    "min_snr_gamma = -1\n",
    "optimizer_type = \"AdamW8bit\"\n",
    "optimizer_args = \"\"\n",
    "train_unet = True\n",
    "unet_lr = 1e-4\n",
    "train_text_encoder = True\n",
    "text_encoder_lr = 5e-5\n",
    "lr_scheduler = \"constant\"\n",
    "lr_warmup_steps = 0\n",
    "lr_scheduler_num_cycles = 0\n",
    "lr_scheduler_power = 0\n",
    "\n",
    "print(\"- LoRA Config:\")\n",
    "print(f\"  - Min-SNR Weighting: {min_snr_gamma}\") if not min_snr_gamma == -1 else \"\"\n",
    "print(f\"  - Loading network module: {network_module}\")\n",
    "print(f\"  - {network_module} linear_dim set to: {network_dim}\")\n",
    "print(f\"  - {network_module} linear_alpha set to: {network_alpha}\")\n",
    "\n",
    "if not network_weight:\n",
    "    print(\"  - No LoRA weight loaded.\")\n",
    "else:\n",
    "    if os.path.exists(network_weight):\n",
    "        print(f\"  - Loading LoRA weight: {network_weight}\")\n",
    "    else:\n",
    "        print(f\"  - {network_weight} does not exist.\")\n",
    "        network_weight = \"\"\n",
    "\n",
    "print(\"- Optimizer Config:\")\n",
    "print(f\"  - Additional network category: {network_category}\")\n",
    "print(f\"  - Using {optimizer_type} as Optimizer\")\n",
    "if optimizer_args:\n",
    "    print(f\"  - Optimizer Args: {optimizer_args}\")\n",
    "if train_unet and train_text_encoder:\n",
    "    print(\"  - Train UNet and Text Encoder\")\n",
    "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
    "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
    "if train_unet and not train_text_encoder:\n",
    "    print(\"  - Train UNet only\")\n",
    "    print(f\"    - UNet learning rate: {unet_lr}\")\n",
    "if train_text_encoder and not train_unet:\n",
    "    print(\"  - Train Text Encoder only\")\n",
    "    print(f\"    - Text encoder learning rate: {text_encoder_lr}\")\n",
    "print(f\"  - Learning rate warmup steps: {lr_warmup_steps}\")\n",
    "print(f\"  - Learning rate Scheduler: {lr_scheduler}\")\n",
    "if lr_scheduler == \"cosine_with_restarts\":\n",
    "    print(f\"  - lr_scheduler_num_cycles: {lr_scheduler_num_cycles}\")\n",
    "elif lr_scheduler == \"polynomial\":\n",
    "    print(f\"  - lr_scheduler_power: {lr_scheduler_power}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set storage format for models, we use safetensors format to adapt to WebUI\n",
    "# Set test prompt\n",
    "# Set training parameters\n",
    "\n",
    "\n",
    "import toml\n",
    "import os\n",
    "\n",
    "lowram = True\n",
    "enable_sample_prompt = True\n",
    "sampler = \"ddim\"  #[\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
    "noise_offset = 0.0\n",
    "num_epochs = 10\n",
    "vae_batch_size = 4\n",
    "train_batch_size = 6\n",
    "mixed_precision = \"fp16\"  # [\"no\",\"fp16\",\"bf16\"]\n",
    "save_precision = \"fp16\"  #  [\"float\", \"fp16\", \"bf16\"]\n",
    "save_n_epochs_type = \"save_every_n_epochs\"\n",
    "save_n_epochs_type_value = 1\n",
    "save_model_as = \"safetensors\"  # [\"ckpt\", \"pt\", \"safetensors\"]\n",
    "max_token_length = 225\n",
    "clip_skip = 2\n",
    "gradient_checkpointing = False\n",
    "gradient_accumulation_steps = 1\n",
    "seed = -1\n",
    "logging_dir = os.path.join(root_dir, \"LoRA/logs\")\n",
    "prior_loss_weight = 1.0\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "# sample_str = f\"\"\"\n",
    "#   masterpiece, best quality, a woman with a very short haircut and a pink shirt, looking at viewer, simple background \\\n",
    "#   --n lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry \\\n",
    "#   --w 512 \\\n",
    "#   --h 512 \\\n",
    "#   --l 7 \\\n",
    "#   --s 28\n",
    "# \"\"\"\n",
    "\n",
    "sample_str = f\"\"\"\n",
    "  masterpiece, best quality, 1girl,moyou，very short haircut and a pink shirt, looking at viewer, simple background \\\n",
    "  --n EasyNegativeV2,(badhandv4:1.2), \\\n",
    "  --w 512 \\\n",
    "  --h 512 \\\n",
    "  --l 7 \\\n",
    "  --s 28\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"model_arguments\": {\n",
    "        \"v2\": False,\n",
    "        \"v_parameterization\": False,\n",
    "        \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
    "        \"vae\": vae,\n",
    "    },\n",
    "    \"additional_network_arguments\": {\n",
    "        \"no_metadata\": False,\n",
    "        \"unet_lr\": float(unet_lr) if train_unet else None,\n",
    "        \"text_encoder_lr\": float(text_encoder_lr) if train_text_encoder else None,\n",
    "        \"network_weights\": network_weight,\n",
    "        \"network_module\": network_module,\n",
    "        \"network_dim\": network_dim,\n",
    "        \"network_alpha\": network_alpha,\n",
    "        \"network_args\": network_args,\n",
    "        \"network_train_unet_only\": True if train_unet and not train_text_encoder else False,\n",
    "        \"network_train_text_encoder_only\": True if train_text_encoder and not train_unet else False,\n",
    "        \"training_comment\": None,\n",
    "    },\n",
    "    \"optimizer_arguments\": {\n",
    "        \"min_snr_gamma\": min_snr_gamma if not min_snr_gamma == -1 else None,\n",
    "        \"optimizer_type\": optimizer_type,\n",
    "        \"learning_rate\": unet_lr,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"optimizer_args\": eval(optimizer_args) if optimizer_args else None,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "        \"lr_warmup_steps\": lr_warmup_steps,\n",
    "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
    "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
    "    },\n",
    "    \"dataset_arguments\": {\n",
    "        \"cache_latents\": True,\n",
    "        \"debug_dataset\": False,\n",
    "        \"vae_batch_size\": vae_batch_size,\n",
    "    },\n",
    "    \"training_arguments\": {\n",
    "        \"output_dir\": output_dir,\n",
    "        \"output_name\": project_name,\n",
    "        \"save_precision\": save_precision,\n",
    "        \"save_every_n_epochs\": save_n_epochs_type_value if save_n_epochs_type == \"save_every_n_epochs\" else None,\n",
    "        \"save_n_epoch_ratio\": save_n_epochs_type_value if save_n_epochs_type == \"save_n_epoch_ratio\" else None,\n",
    "        \"save_last_n_epochs\": None,\n",
    "        \"save_state\": None,\n",
    "        \"save_last_n_epochs_state\": None,\n",
    "        \"resume\": None,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"max_token_length\": 225,\n",
    "        \"mem_eff_attn\": False,\n",
    "        \"xformers\": True,\n",
    "        \"max_train_epochs\": num_epochs,\n",
    "        \"max_data_loader_n_workers\": 8,\n",
    "        \"persistent_data_loader_workers\": True,\n",
    "        \"seed\": seed if seed > 0 else None,\n",
    "        \"gradient_checkpointing\": gradient_checkpointing,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"mixed_precision\": mixed_precision,\n",
    "        \"clip_skip\": clip_skip,\n",
    "        \"logging_dir\": logging_dir,\n",
    "        \"log_prefix\": project_name,\n",
    "        \"noise_offset\": noise_offset if noise_offset > 0 else None,\n",
    "        \"lowram\": lowram,\n",
    "    },\n",
    "    \"sample_prompt_arguments\": {\n",
    "        \"sample_every_n_steps\": None,\n",
    "        \"sample_every_n_epochs\": 1 if enable_sample_prompt else 999999,\n",
    "        \"sample_sampler\": sampler,\n",
    "    },\n",
    "    \"dreambooth_arguments\": {\n",
    "        \"prior_loss_weight\": 1.0,\n",
    "    },\n",
    "    \"saving_arguments\": {\n",
    "        \"save_model_as\": save_model_as\n",
    "    },\n",
    "}\n",
    "\n",
    "config_path = os.path.join(config_dir, \"config_file.toml\")\n",
    "prompt_path = os.path.join(config_dir, \"sample_prompt.txt\")\n",
    "\n",
    "\n",
    "for key in config:\n",
    "    if isinstance(config[key], dict):\n",
    "        for sub_key in config[key]:\n",
    "            if config[key][sub_key] == \"\":\n",
    "                config[key][sub_key] = None\n",
    "    elif config[key] == \"\":\n",
    "        config[key] = None\n",
    "\n",
    "config_str = toml.dumps(config)\n",
    "\n",
    "write_file(config_path, config_str)\n",
    "write_file(prompt_path, sample_str)\n",
    "\n",
    "print(config_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## During the process you can see generated images during training in LoRA/output/hb_pro/sample directory. For selecting your satisfactory LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = os.path.join(config_dir, \"sample_prompt.txt\")\n",
    "config_file = os.path.join(config_dir, \"config_file.toml\")\n",
    "dataset_config = os.path.join(config_dir, \"dataset_config.toml\")\n",
    "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "\n",
    "accelerate_conf = {\n",
    "    \"config_file\" : accelerate_config,\n",
    "    \"num_cpu_threads_per_process\" : 1,\n",
    "}\n",
    "\n",
    "train_conf = {\n",
    "    \"sample_prompts\" : sample_prompt,\n",
    "    \"dataset_config\" : dataset_config,\n",
    "    \"config_file\" : config_file\n",
    "}\n",
    "\n",
    "def train(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    return args\n",
    "\n",
    "accelerate_args = train(accelerate_conf)\n",
    "train_args = train(train_conf)\n",
    "final_args = f\"accelerate launch {accelerate_args} train_network.py {train_args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LoRA effects for different epochs\n",
    "pths = sorted(glob.glob(r\"/content/LoRA/output/hb_pro/sample/*\"))\n",
    "print(pths)\n",
    "\n",
    "imgs = []\n",
    "for pth in pths:\n",
    "  img = Image.open(pth)\n",
    "  imgs.append(img)\n",
    "\n",
    "image_grid(imgs, 2, len(imgs)//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The safetensor files in LoRA/output/hb_pro/ directory are the obtained LoRA models, download to WebUI can be used directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "network_weight = \"/content/LoRA/output/hb_pro/hb_pro.safetensors\"\n",
    "network_mul = 1\n",
    "network_module = \"networks.lora\"\n",
    "network_args = \"\"\n",
    "\n",
    "v2 = False\n",
    "v_parameterization = False\n",
    "prompt = \"masterpiece, best quality, 1girl moyou ( ink sketch) fantasy, surreal muted color ( Russ Mills Anna Dittmann)\"   # 你要测试的prompt\n",
    "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "model = pretrained_model_name_or_path\n",
    "vae = \"\"\n",
    "outdir = \"/content/tmp\"  # Image storage path\n",
    "scale = 7\n",
    "sampler = \"euler_a\"\n",
    "steps = 20\n",
    "precision = \"fp16\"\n",
    "width = 512\n",
    "height = 768\n",
    "images_per_prompt = 4\n",
    "batch_size = 4\n",
    "clip_skip = 2\n",
    "seed = 1024\n",
    "\n",
    "final_prompt = f\"{prompt} --n {negative}\"\n",
    "\n",
    "config = {\n",
    "    \"v2\": v2,\n",
    "    \"v_parameterization\": v_parameterization,\n",
    "    \"network_module\": network_module,\n",
    "    \"network_weight\": network_weight,\n",
    "    \"network_mul\": float(network_mul),\n",
    "    \"network_args\": eval(network_args) if network_args else None,\n",
    "    \"ckpt\": model,\n",
    "    \"outdir\": outdir,\n",
    "    \"xformers\": True,\n",
    "    \"vae\": vae if vae else None,\n",
    "    \"fp16\": True,\n",
    "    \"W\": width,\n",
    "    \"H\": height,\n",
    "    \"seed\": seed if seed > 0 else None,\n",
    "    \"scale\": scale,\n",
    "    \"sampler\": sampler,\n",
    "    \"steps\": steps,\n",
    "    \"max_embeddings_multiples\": 3,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"images_per_prompt\": images_per_prompt,\n",
    "    \"clip_skip\": clip_skip if not v2 else None,\n",
    "    \"prompt\": final_prompt,\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python gen_img_diffusers.py {args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test images\n",
    "pths = sorted(glob.glob(r\"/content/tmp/*\"))\n",
    "print(pths)\n",
    "\n",
    "imgs = []\n",
    "for pth in pths[-4:]:\n",
    "  img = Image.open(pth)\n",
    "  imgs.append(img)\n",
    "\n",
    "image_grid(imgs, 1, len(imgs))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
